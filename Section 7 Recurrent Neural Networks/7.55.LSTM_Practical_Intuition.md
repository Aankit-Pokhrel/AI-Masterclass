# LSTM Practical Intuition

## How LSTMs Work inside practical applications

To start off we will look at this Architecture, specifically the right most tangent function, and how it fires up

### tanh

![LSTM](./7.55.1.jpg)

Let's start with some examples:

_keep in mind for this part of the section +1 blue, and -1 is red_

![LSTMText](./7.55.2.jpg)

This is some text given to an RNN that has learned to read and create text, and predict what text is coming

for the portion labelled "Cell sensitive to position in line"

- the Neuron is sensitive to the position in the line
  - how does it know it is at the end of the line?
    - The novel has approx. 80 symbols per line, so it just counts, and it predicts when the \n character pops up

for the portion labelled "Cell that turns on inside quotes"

- the neuron returns blue when in quotes, and red outside of the quotes
  - it keeps track of the quotes

![LSTMCode](./7.55.3.jpg)

for the portion labelled "Cell that robustly activates inside if statements"

- the code is the input
  - it activates inside an if statement (just the condition portion,)
  - How can it tell?
    - Looks for if and an open bracket, then a matching closing bracket

for the portion labelled "Cell that is sensitive to the depth of an expression"

- the code is the input
  - Activates once it gets into an expression, as the depth is increasing and the expression is more nested, it uses the memory to keep track of it

_it is very important to remember that this is learned behaviour, not hard coded_

![Hard to tell](./7.55.4.jpg)

for this example, it isn't easy to understand what it is doing, similar to after convolution in CNNs

### Output

Now that we have looked through the output gate, lets look at the output being produced at h<sub>t</sub>

![RNN](./7.55.5.jpg)

So we have this from [karpathy.github.io](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)

![Reading NN](./7.55.6.jpg)

in the top line of each of the sections, the NN is reading, and then after that it predicts what it thinks would be next.

- Green means active
- Blue means inactive
- Red means likely prediction
- Light red means less likely prediction

What do we think it is predicting

- for the top line what is it reading?
  - it activates inside URLs
- in the lines after that
  - for each index of the top line, in the second like it predicts what it thinks the next character would be
  - for the lighter ones, the NN isn't super confident about that prediction
- the rows from 2-5 go in odrer of it's predictions, and they are ordered best to worst guess

# Additional reading

[Visualizing and understanding Recurrent Networks](https://arxiv.org/pdf/1506.02078)

- provides insights
- they read specific neurons
