# Backpropagation

the process to get the ŷ is called Forward Propagation

Backpropagation is the process in which all of the weights are updated from the cost function, in the opposite direction from Forward Propagation

Backpropagation is an advanced algorithm which allows us to adjust all of the weights simultaneously

the advantage of this is that during the process of backpropagation, due to the way the algorithm is structured, you are able to adjust all of the weights at the same time, and so you basically know which part of the error, each of the weights in your NN is responsible for

this is why it was widely picked up in the 1980s

## Steps of training the NN with SGD

1. Randomly initialize the weights with numbers close to 0, but not = 0
2. input the first observation of your dataset into the input layer, each feature is 1 input node
3. Forward Propagation: from left to right, the neurons are activated in a way that the impact of each neuron's activation is limited by the weights. Propagate the activations until getting the predicted result of y (ŷ)
4. compare the predicted result from the actual result. Measure the generated error (done with Cost Function)
5. Back Propagation: from right to left, the error is back propagated. Update the weights according to how much they are responsible for the error. the learning rate decides by how much we update the weights
6. - Repeat Steps 1 - 5 and update the weights after each observation (Reinforcement Learning) (SGD)
   - Repeat Steps 1 - 5 but update the weights only after a batch of observations (Batch Learning) (GD or MGD)
7. When the whole training set passed through the NN, that makes an Epoch. Redo more Epochs (This step improves the NN, and allows it to constantly adjust itself as you minimize the cost function)
