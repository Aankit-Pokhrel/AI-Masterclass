# Stochastic Gradient Descent

Gradient Descent is an efficient method of minimizing our C value

the issue with Gradient Descent is that it requires the Cost Function to be Convex such as the following image
![Gradient Descent 2](./Gradient%20Descent%202.png)

but the cost function may not be convex such as the following image

![Stochastic Gradient Descent 1](./Stochastic%20Gradient%20Descent%201.png)

this can happen if there is more than 1 dimension, or if we chose a cost function with more than 1 local minimum

if we apply our normal gradient descent method, we can find a local minimum, that might not be the global minimum

![Stochastic Gradient Descent 2](./Stochastic%20Gradient%20Descent%202.png)

the consequence is that we do not have an optimized NN, the NN will be subpar

<span style="font-weight: 500;">the answer to this issue is Stochastic Gradient Descent<span/>

Stochastic Gradient descent does not require the cost function to be convex

### What is the difference between Gradient Descent (GD) and Stochastic Gradient Descent (SGD)?

##### GD

Normal GD is when we take all of our rows of data, plug them into our NN, calculate the cost function, and adjust the weights accordingly

![Stochastic Gradient Descent 3](./Stochastic%20Gradient%20Descent%203.png)

##### SGD

SGD is when the weights are adjusted after each row of data is plugged in and run

this is done for each row of data

![Stochastic Gradient Descent 4](./Stochastic%20Gradient%20Descent%204.png)

#### Visual

###### Batch GD

![Stochastic Gradient Descent 5](./Stochastic%20Gradient%20Descent%205.png)

this is a deterministic algorithm

as long as the starting weights are the same, every time this method is run, the weight updates will be the same

###### SGD

![Stochastic Gradient Descent 6](./Stochastic%20Gradient%20Descent%206.png)

_in SGD the epoch is still when all of the data is processed once, not when each row is run_

this is a stochastic (meaning random) algorithm

Even if the starting weights are the same, every time this method is run, the weight updates will most likely not be the same

this is lighter since you don't need to load all of the data at once

the rows may be picked at random

### Extra

There is another method called the Mini Batch Gradient Descent method (MGD), where the batch is broken up into sets of rows, and the algorithm is run for the amount of sets

only MGD if the set sizes are > 1
