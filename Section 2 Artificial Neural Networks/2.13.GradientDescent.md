# Gradient Descent

Gradient descent is the process of exactly how the weights are adjusted

this is our very simple single perceptron neural network

![Gradient Descent 1](./Gradient%20Descent%201.png)

we can see the whole process in action

how can we minimize the cost function

- one approach is to brute force it
  - this is the simplest approach
  - this is less efficient if there are more weights or more synapses
    - this issue is commonly called the curse of dimensionality
      - the curse of dimensionality describes the time issue as the amount of weights and synapses increases
      - Example:
        - a simple 4 input 5 synapse NN before training will have 25 weights
        - if there are 1000 combinations that will be tested for each weight, that means there will be $$10^{75}$$combinations, which even the worlds fastest supercomputer cannot do in a reasonable timeframe

NNs get more complex than the example above

the method that we will cover is called gradient descent

![Gradient Descent 2](./Gradient%20Descent%202.png)

- this is done by looking at the gradient (slope) of the cost function at a certain point
  - if the slope is (-) then the cost function value needs to become more positive
  - if the slope is (+) then the cost function value needs to become more negative

this is more efficient because you have less steps than brute forcing
